# -*- coding: utf-8 -*-
"""Heart-Disease-Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m2AdMnOYAAX_MH_GrUZ8pB75-DW9YESx

# Predicting Heart Disease using machine learning

This notebook looks into using various Python-based machine learning and data science libraries in an attempt to build a machine learning model capable of predicting whether or not someone has heart disease based on their medical attributes.


We're going to take the following approach:


1. Problem definition
2. Data
3. Evaluation
4. Features
5. Modelling
6. Experimentation

## 1. Problem Definition

In a statement,
> Given clinical parameters about a patient, can w epredict whether or not they have heart disease?

## 2. Data

The original data can be seen from the link attached.

## 3. Evaluation

> If we can reach 95% Accuracy at predicting whether or not a patient has heart disease during the proof of concept, we'll paursue the project.

## 4. Features

This is where you'll get different information about each of the features in your data.

**Create data dictionary**


* age: Displays the age of the individual.

* sex: Displays the gender of the individual using the following format : 1 = male 0 = female

* cp- Chest-pain type: displays the type of chest-pain experienced by the individual using the following format : 0 = typical angina 1 = atypical angina 2 = non — anginal pain 3 = asymptotic

* trestbps- Resting Blood Pressure: displays the resting blood pressure value of an individual in mmHg (unit). anything above 130-140 is typically cause for concern.

* chol- Serum Cholestrol: displays the serum cholesterol in mg/dl (unit)

* fbs- Fasting Blood Sugar: compares the fasting blood sugar value of an individual with 120mg/dl. If fasting blood sugar > 120mg/dl then : 1 (true) else : 0 (false) '>126' mg/dL signals diabetes

* restecg- Resting ECG : displays resting electrocardiographic results 0 = normal 1 = having ST-T wave abnormality 2 = left ventricular hyperthrophy

* thalach- Max heart rate achieved : displays the max heart rate achieved by an individual.

* exang- Exercise induced angina : 1 = yes 0 = no

* oldpeak- ST depression induced by exercise relative to rest: displays the value which is an integer or float.

* slope- Slope of the peak exercise ST segment : 0 = upsloping: better heart rate with excercise (uncommon) 1 = flat: minimal change (typical healthy heart) 2 = downsloping: signs of unhealthy heart

* ca- Number of major vessels (0–3) colored by flourosopy : displays the value as integer or float.

* thal : Displays the thalassemia : 1,3 = normal 6 = fixed defect 7 = reversible defect: no proper blood movement when excercising

* target : Displays whether the individual is suffering from heart disease or not : 1 = yes 0 = no

## Preparing the tools

We're going to use Pandas, NumPy and Matplotlib for Data Analysis and Manipulation.
"""

# Commented out IPython magic to ensure Python compatibility.
# Import all the tools we need

# Regular EDA (Explolatory Data Analysis) and plotting libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# we want our plots to appear inside the notebook
# %matplotlib inline

# Import models from Scikit-Learn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier

# Model Evaluations
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.metrics import RocCurveDisplay

"""## Load Data"""

df = pd.read_csv('/heart-disease.csv')
df

df.shape # (rows, cols)

"""## Data Exploration (Exploratory Data Analysis Or EDA)

The Goal Here is to find out more about the data and become a subject matter expert on the dataset you're working with.


1. What question(s) are you trying to solve?
2. What kind of data do we have and how do we treat different types?
3. What's missing from the data and how to deal with it?
4. What are the outliers and why should you care about them?
5. How can you add, change or remove features to get more out of your data?
"""

df.head()

df.tail()

# Let's find out how many of each class are
df["target"].value_counts()

df["target"].value_counts().plot(kind="bar", color=["salmon", "lightblue"]);

df.info()

# Are there any missing values?
df.isna().sum()

# More description about our data.
df.describe()

"""### Heart Disease Frequency According To Sex"""

df.sex.value_counts()

# Compare target column with sex column
pd.crosstab(df.target, df.sex)

# Create a plot of crosstab with (0 as no heart disease and 0 as sex means female)
pd.crosstab(df.target, df.sex).plot(kind="bar", figsize=(10, 6), color=["salmon", "lightblue"]);

plt.title("Heart Disease Frequency for Sex")
plt.xlabel("0 = No Disease, 1 = Disease")
plt.ylabel("Amount")
plt.legend(["Female", "Male"]);
plt.xticks(rotation=0);

# Max heart rate achieved.
df["thalach"].value_counts()

"""### Age vs. Max Heart Rate for Heart Disease"""

# Creating another figure
plt.figure(figsize=(10, 6))

# Scatter with positive examples
plt.scatter(df.age[df.target==1],
            df.thalach[df.target==1],
            c="salmon"
            )

# Scatter with negative examples
plt.scatter(df.age[df.target==0],
            df.thalach[df.target==0],
            c="lightblue");

# Add some helpful info
plt.title('Heart Disease in function of Age and Max Heart Rate')
plt.xlabel('Age')
plt.ylabel('Max Heart Rate')
plt.legend(['Disease', 'No Disease'])

# Check the distribution of the Age column with a histogram
# Very close to the normal distribution
df.age.plot.hist();

"""### Heart Disease Frequency per chest pain type"""

pd.crosstab(df.cp, df.target)

# Make the crosstab more visual
pd.crosstab(df.cp, df.target).plot(kind="bar", figsize=(10, 6), color=["salmon", "lightblue"])

# Add some communication
plt.title("Heart Disease Frequency Per Chest Pain Type")
plt.xlabel("Chest Pain Type")
plt.ylabel("Amount")
plt.legend(["No Disease", "Disease"])
plt.xticks(rotation=0);

# More heart disease to chest pain type 2

df.head()

# Make a correlation Matrix
df.corr()

"""#### Negative correlation = a relationship between two variables in which one variable increases as the other decreases."""

# Let's make our correlation matrix a little prettier
corr_matrix = df.corr()
fig, ax = plt.subplots(figsize=(15, 10))
ax = sns.heatmap(corr_matrix, annot=True, linewidths=0.5, fmt=".2f", cmap="YlGnBu")

bottom, top = ax.get_ylim()
ax.set_ylim(bottom+0.5, top-0.5)

"""## 5. Modelling"""

df.head()

# Split data into x and y
X = df.drop("target", axis=1)
y = df["target"]

X

y

# Split data into train and test sets
np.random.seed(42)

# Split into train & test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

X_train

y_train, len(y_train)

"""Now we've got our data split into training and test sets, it's time to build a machine learning model.

We'll train it (find the patterns) on the training set.

And we'll test it (use the patterns) on the test set.

 We're going to try different machine learning models:
 1. Logistic Regression
 2. K-Nearest Neighbous Classifier
 3. Random Forest Classifier
"""

# Put models in dictionary
models = {"Logistic Regression": LogisticRegression(),
          "KNN": KNeighborsClassifier(),
          "Random Forest": RandomForestClassifier()}

# Create a function to fit and score models
def fit_and_score(models, X_train, X_test, y_train, y_test):
  """
  Fits and evaluates given machine learning models.
  models: a dict of different Scikit-learn Machine learning models
  X_train: training data (no labels)
  X_test: testing data (no labels)
  y_train: training labels
  y_test: test labels
  """

  # Set random seed
  np.random.seed(42)

  # Make a dictionary to keep model scores
  model_scores = {}

  # Loop through models
  for name, model in models.items():
    # Fit the model to the data
    model.fit(X_train, y_train)

    # Evaluate the model and append its score to model_scores
    model_scores[name] = model.score(X_test, y_test)

  return model_scores

model_scores = fit_and_score(models=models, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)

model_scores

"""### Model Comparison"""

model_compare = pd.DataFrame(model_scores, index=["accuracy"])
model_compare.T.plot.bar()

"""Now we've got a baseline model... and we know a model's first predictions aren't always what we should based our next steps off. What should do?

Let's look at the following:
* Hyperparameter tuning
* Feature importance
* Confusion matrix
* Cross-validation
* Precision
* Recall
* F1 score
* Classification Report
* ROC Curve
* Area under the curve (AUC)

## Hyperparameter Tuning (By Hand)
"""

# Let's tune KNN

train_scores = []
test_scores = []

# Create a list of different values for n_neighbors
neighbors = range(1, 21)

# Setup KNN instance
knn = KNeighborsClassifier()

# Loop through different n_neighbors
for i in neighbors:
  knn.set_params(n_neighbors=i)

  # Fit the algorithm
  knn.fit(X_train, y_train)

  # Update the training scores list
  train_scores.append(knn.score(X_train, y_train))

  # Update the test scores list
  test_scores.append(knn.score(X_test, y_test))

train_scores

test_scores

plt.plot(neighbors, train_scores, label="Train Score")
plt.plot(neighbors, test_scores, label='Test Score')
plt.xticks(np.arange(1, 21, 1))
plt.xlabel("Number of neighbors")
plt.ylabel("Model score")
plt.legend()

print(f"Maximum KNN Score on the test data: {max(test_scores)*100:.2f}%")

"""Although we did the hyperparameter tuning, still we can't achieve the score we wanted to achieve. So we will discard the KNN Algorithm.

## Hyperparameter Tuning With RandomizedSearchCV

We're going to tune:
* LogisticRegression()
* RandomForestClassifier()

... using RandomizedSearchCV
"""

# Create a hyperparameter grid for LogisticRegression
log_reg_grid = {"C": np.logspace(-4, 4, 20), "solver": ["liblinear"]}

# Create a hyperparamter grid for RandomForestClassifier
rf_grid = {"n_estimators": np.arange(10, 1000, 50),
            "max_depth": [None, 3, 5, 10],
            "min_samples_split": np.arange(2, 20, 2),
           "min_samples_leaf": np.arange(1, 20, 2)
           }

np.arange(10, 1000, 50)

"""Now we've got hyperparameter grids setup for each of our models, let's tune them using RandomizedSearchCV..."""

# Tune LogisticRegression

np.random.seed(42)

# Setup random hyperparameter search for LogisticRegression
rs_log_reg = RandomizedSearchCV(LogisticRegression(), param_distributions=log_reg_grid, cv=5, n_iter=20, verbose=True)

# Fit random hyperparameter search model for LogisticRegression
rs_log_reg.fit(X_train, y_train)

rs_log_reg.best_params_

rs_log_reg.score(X_test, y_test)

"""Now we've tuned LogisticRegression(), let's do the same for RandomForestClassifier"""

# Setup random seed
np.random.seed(42)

# Setup random hyperparamter search for RandomForestClassifier
rs_rf = RandomizedSearchCV(RandomForestClassifier(), param_distributions=rf_grid, cv=5, n_iter=20, verbose=True)

# Fit random hyperparameter search model for RandomForestClassfier()
rs_rf.fit(X_train, y_train)

# Find the best hyperparameters

rs_rf.best_params_

# Evaluate the randomized search RandomForestClassifier model
rs_rf.score(X_test, y_test)

model_scores

"""1. By hand
2. RandomizedSearchCV
3. GridSearchCV

## Hyperparameter Tuning with GridSearchCV

Since our LogisticRegression model provides the best scores so far, we'll try and improve them again using GridSearchCV...
"""

# Different hyperparameters for oir LogisticRegression model
log_reg_grid = {"C": np.logspace(-4, 4, 30),
                "solver": ["liblinear"]
              }

# Setup grid hyperparamter search for LogisticRegression
gs_log_reg = GridSearchCV(LogisticRegression(), param_grid = log_reg_grid,
                          cv=5, verbose=True)

# Fit grid hyperparameter search model
gs_log_reg.fit(X_train, y_train);

# Check the best hyperparameters
gs_log_reg.best_params_

# Evaluate the grid search LogisticRegression model
gs_log_reg.score(X_test, y_test)

"""## Evaluating our tuned machine learning classifier, beyond accuracy

* ROC curve and AUC curve
* Confusion matrix
* Classification report
* Precision
* Recall
* F1-score

... and it would be great if cross-validation was used where possible.

To make comparisons and evaluate trained model, first we need to make predictions.
"""

# Make predictions with tuned model
y_preds = gs_log_reg.predict(X_test)

y_preds

y_test

# Import ROC curve function from the sklearn.metrics module

# Plot ROC curve and calculate and calculate AUC metric
RocCurveDisplay.from_estimator(estimator=gs_log_reg,
                               X=X_test,
                               y=y_test);

# Confusion Matrix
print(confusion_matrix(y_test, y_preds))

# Import Seaborn
import seaborn as sns
sns.set(font_scale=1.5) # Increase font size

def plot_conf_mat(y_test, y_preds):
    """
    Plots a confusion matrix using Seaborn's heatmap().
    """
    fig, ax = plt.subplots(figsize=(3, 3))
    ax = sns.heatmap(confusion_matrix(y_test, y_preds),
                     annot=True, # Annotate the boxes
                     cbar=False)
    plt.xlabel("Predicted label") # predictions go on the x-axis
    plt.ylabel("True label") # true labels go on the y-axis

plot_conf_mat(y_test, y_preds)

"""Now we've got a ROC curve, an AUC metric and a confusion matrix, let's get a classification report as well as cross-validated precision, recall and f1-score"""

print(classification_report(y_test, y_preds))

"""## Calculate evaluation metrics using cross-validation

we're going to calculate accuracy, precision, recall and f1-score of our model using cross-validation and to do we'll be using `cross_val_score()`
"""

# Check best hyperparameters
gs_log_reg.best_params_

# Create a new classifier with best parameters
clf = LogisticRegression(C=0.20433597178569418, solver='liblinear')

# Cross-validated accuracy
cv_acc = cross_val_score(clf, X, y, cv=5, scoring="accuracy")

cv_acc

cv_acc = np.mean(cv_acc)
cv_acc

# Cross-validated precision
cv_precision = cross_val_score(clf, X, y, cv=5, scoring="precision")

cv_precision = np.mean(cv_precision)
cv_precision

# Cross-validated recall
cv_recall = cross_val_score(clf, X, y, cv=5, scoring="recall")

cv_recall = np.mean(cv_recall)
cv_recall

# Cross-validated f1-score
cv_f1 = cross_val_score(clf, X, y, cv=5, scoring="f1")

cv_f1 = np.mean(cv_f1)
cv_f1

# Visualize cross-validated metrics
cv_metrics = pd.DataFrame({"Accuracy": cv_acc, "Precision":cv_precision, "Recall":cv_recall, "F1":cv_f1}, index=[0])

cv_metrics.T.plot.bar(title="Cross-validated classification metrics", legend=False)

"""### Feature Importance

Feature importance is another as asking, "which features contributed most to th eoutcomes of the model and how did they contribute?"

Finding feature importance is different for each machine learning model. One way to find feature importance is to search for "(MODEL NAME) feature importance".

Let's find the feature importance for our LogisticRegression model
"""

df.head()



# Fit an instance of LogisticRegression
clf = LogisticRegression(C=0.20433597178569418, solver='liblinear')

clf.fit(X_train, y_train);

# Check coef_
clf.coef_

# Match coef's of features to columns
features_dict = dict(zip(df.columns, list(clf.coef_[0])))
features_dict

# Visualize feature importance
feature_df = pd.DataFrame(features_dict, index=[0])
feature_df.T.plot.bar(title="Feature Importance", legend=False);

pd.crosstab(df["sex"], df["target"])

pd.crosstab(df["slope"], df["target"])

"""# 6. Experimentation

If you haven't hit your evaluation metric yet.. ask yourself...

* Could you collect more data?

* Could you try a better model? If you're working with structured data, you might want to look into CatBoost or XGBoost.

* Could you improve the current models (beyond what we've done so far)?

* If your model is good enough, how would you export it and share it with others? (Hint: check out Scikit-Learn's documentation on model persistance)
"""

